{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW4tO4nD+id0dpgrsOlfgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwasswa2023/Physpropnet/blob/main/GNN_GATMODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITANEOsjKqXd"
      },
      "outputs": [],
      "source": [
        "# Install CPU-only PyTorch 2.0.1 and matching torchdata\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 torchdata==0.6.1\n",
        "\n",
        "# Install DeepChem (Torch-compatible)\n",
        "!pip install \"numpy==2.3.1\" \"deepchem[torch]\"\n",
        "\n",
        "# Install PyTorch Geometric (CPU-only)\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cpu.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Install DGL (CPU version)\n",
        "!pip install dgl==1.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchdata\n",
        "import torch_geometric\n",
        "import dgl\n",
        "import deepchem as dc\n",
        "import numpy as np\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"TorchData:\", torchdata.__version__)\n",
        "print(\"TorchGeometric:\", torch_geometric.__version__)\n",
        "print(\"DGL:\", dgl.__version__)\n",
        "print(\"DeepChem:\", dc.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n"
      ],
      "metadata": {
        "id": "yYmH0i8UK06o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96HsOcvaLLpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTm24vWLeQeH",
        "outputId": "7fc7355e-ce3f-4560-d0a2-256eb88b3d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n",
            "\n",
            "Fold 2/5\n",
            "\n",
            "Fold 3/5\n",
            "\n",
            "Fold 4/5\n",
            "\n",
            "Fold 5/5\n",
            "\n",
            "=== Cross-Validation Summary ===\n",
            "RÂ²     : 0.8678 Â± 0.0146\n",
            "RMSE   : 0.7402 Â± 0.0145\n",
            "MAE    : 0.5393 Â± 0.0127\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from deepchem.feat import PagtnMolGraphFeaturizer\n",
        "from deepchem.data import NumpyDataset\n",
        "from deepchem.models.torch_models import GATModel\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/desalted_WS.csv\")  # Adjust path as needed\n",
        "smiles_list = df[\"SMILES\"].tolist()\n",
        "labels = df[\"LogWS\"].values\n",
        "\n",
        "# Featurize molecules\n",
        "featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "features = featurizer.featurize(smiles_list)\n",
        "\n",
        "# Filter out failed featurizations\n",
        "valid_data = [(f, l, s) for f, l, s in zip(features, labels, smiles_list) if f is not None]\n",
        "X, y, ids = zip(*valid_data)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "ids = np.array(ids)\n",
        "\n",
        "# Perform the initial train/validation/test split on the data arrays\n",
        "# Split into 80% train_val and 20% test\n",
        "X_train_val, X_test, y_train_val, y_test, ids_train_val, ids_test = train_test_split(\n",
        "    X, y, ids, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Create DeepChem datasets for train_val and test\n",
        "train_val_dataset = NumpyDataset(X=X_train_val, y=y_train_val, ids=ids_train_val)\n",
        "test_dataset = NumpyDataset(X=X_test, y=y_test, ids=ids_test)\n",
        "\n",
        "\n",
        "# Cross-validation setup on training+validation set\n",
        "num_folds = 5\n",
        "# KFold splits indices based on the number of samples in the dataset\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Collect metrics\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "mae_scores = []\n",
        "\n",
        "# Cross-validation loop\n",
        "# Use kf.split on the indices of the train_val_dataset\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_val_dataset.ids)):\n",
        "    print(f\"\\nFold {fold+1}/{num_folds}\")\n",
        "\n",
        "    # Select data for the current fold's train and validation sets using .select()\n",
        "    train_dataset = train_val_dataset.select(train_idx)\n",
        "    valid_dataset = train_val_dataset.select(valid_idx)\n",
        "\n",
        "    # Re-instantiate model for each fold\n",
        "    model = GATModel(1, mode=\"regression\", batch_normalize=False)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(train_dataset, nb_epoch=30)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    preds = model.predict(valid_dataset)\n",
        "    true = valid_dataset.y\n",
        "\n",
        "    r2 = r2_score(true, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(true, preds))\n",
        "    mae = mean_absolute_error(true, preds)\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    rmse_scores.append(rmse)\n",
        "    mae_scores.append(mae)\n",
        "\n",
        "# Report mean Â± std for each metric\n",
        "print(\"\\n=== Cross-Validation Summary ===\")\n",
        "print(f\"RÂ²     : {np.mean(r2_scores):.4f} Â± {np.std(r2_scores):.4f}\")\n",
        "print(f\"RMSE   : {np.mean(rmse_scores):.4f} Â± {np.std(rmse_scores):.4f}\")\n",
        "print(f\"MAE    : {np.mean(mae_scores):.4f} Â± {np.std(mae_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHwJkcuiLOzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krsd2z2eYEuU",
        "outputId": "d3e73cf8-bade-43fb-c803-bac3e42724f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” Iteration 1/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8475\n",
            "\n",
            "ğŸ” Iteration 2/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.2, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8351\n",
            "\n",
            "ğŸ” Iteration 3/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 2, 'agg_modes': ['flatten', 'flatten'], 'residual': False, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9613\n",
            "\n",
            "ğŸ” Iteration 4/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 12, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8837\n",
            "\n",
            "ğŸ” Iteration 5/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 8, 'agg_modes': ['mean', 'flatten'], 'residual': False, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.7584\n",
            "\n",
            "ğŸ” Iteration 6/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9482\n",
            "\n",
            "ğŸ” Iteration 7/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9552\n",
            "\n",
            "ğŸ” Iteration 8/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 12, 'agg_modes': None, 'residual': False, 'dropout': 0.0, 'alpha': 0.2, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9208\n",
            "\n",
            "ğŸ” Iteration 9/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 10, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.0, 'alpha': 0.2, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9264\n",
            "\n",
            "ğŸ” Iteration 10/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 4, 'agg_modes': ['mean', 'flatten'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9312\n",
            "\n",
            "ğŸ” Iteration 11/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 10, 'agg_modes': None, 'residual': True, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9242\n",
            "\n",
            "ğŸ” Iteration 12/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9046\n",
            "\n",
            "ğŸ” Iteration 13/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['mean', 'flatten'], 'residual': True, 'dropout': 0.0, 'alpha': 0.2, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9427\n",
            "\n",
            "ğŸ” Iteration 14/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 6, 'agg_modes': None, 'residual': False, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8623\n",
            "\n",
            "ğŸ” Iteration 15/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'flatten'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9163\n",
            "\n",
            "ğŸ” Iteration 16/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 12, 'agg_modes': ['mean', 'flatten'], 'residual': False, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9071\n",
            "\n",
            "ğŸ” Iteration 17/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 10, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9037\n",
            "\n",
            "ğŸ” Iteration 18/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['mean', 'flatten'], 'residual': False, 'dropout': 0.4, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8488\n",
            "\n",
            "ğŸ” Iteration 19/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 6, 'agg_modes': ['mean', 'flatten'], 'residual': True, 'dropout': 0.0, 'alpha': 0.2, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8872\n",
            "\n",
            "ğŸ” Iteration 20/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 2, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.5318\n",
            "\n",
            "ğŸ” Iteration 21/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 2, 'agg_modes': ['mean', 'mean'], 'residual': False, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8804\n",
            "\n",
            "ğŸ” Iteration 22/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 10, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8602\n",
            "\n",
            "ğŸ” Iteration 23/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 4, 'agg_modes': ['mean', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8688\n",
            "\n",
            "ğŸ” Iteration 24/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'flatten'], 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.7829\n",
            "\n",
            "ğŸ” Iteration 25/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 12, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9293\n",
            "\n",
            "ğŸ” Iteration 26/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 4, 'agg_modes': None, 'residual': True, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8869\n",
            "\n",
            "ğŸ” Iteration 27/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 12, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.2, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9231\n",
            "\n",
            "ğŸ” Iteration 28/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 12, 'agg_modes': None, 'residual': True, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9414\n",
            "\n",
            "ğŸ” Iteration 29/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 10, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8147\n",
            "\n",
            "ğŸ” Iteration 30/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 2, 'agg_modes': ['flatten', 'flatten'], 'residual': True, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8768\n",
            "\n",
            "ğŸ” Iteration 31/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 10, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.2, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8973\n",
            "\n",
            "ğŸ” Iteration 32/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 12, 'agg_modes': ['flatten', 'flatten'], 'residual': True, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8887\n",
            "\n",
            "ğŸ” Iteration 33/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 12, 'agg_modes': None, 'residual': True, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9423\n",
            "\n",
            "ğŸ” Iteration 34/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9344\n",
            "\n",
            "ğŸ” Iteration 35/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 2, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8466\n",
            "\n",
            "ğŸ” Iteration 36/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.2, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8665\n",
            "\n",
            "ğŸ” Iteration 37/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 4, 'agg_modes': ['flatten', 'flatten'], 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.0872\n",
            "\n",
            "ğŸ” Iteration 38/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 2, 'agg_modes': None, 'residual': True, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9411\n",
            "\n",
            "ğŸ” Iteration 39/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 4, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8690\n",
            "\n",
            "ğŸ” Iteration 40/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 4, 'agg_modes': None, 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8309\n",
            "\n",
            "ğŸ” Iteration 41/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 12, 'agg_modes': ['mean', 'flatten'], 'residual': False, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8631\n",
            "\n",
            "ğŸ” Iteration 42/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8857\n",
            "\n",
            "ğŸ” Iteration 43/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 10, 'agg_modes': ['mean', 'flatten'], 'residual': True, 'dropout': 0.0, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9504\n",
            "\n",
            "ğŸ” Iteration 44/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 4, 'agg_modes': None, 'residual': True, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9579\n",
            "\n",
            "ğŸ” Iteration 45/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 10, 'agg_modes': None, 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9533\n",
            "\n",
            "ğŸ” Iteration 46/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 6, 'agg_modes': ['flatten', 'flatten'], 'residual': False, 'dropout': 0.2, 'alpha': 0.1, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8461\n",
            "\n",
            "ğŸ” Iteration 47/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 4, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.2, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8428\n",
            "\n",
            "ğŸ” Iteration 48/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'flatten'], 'residual': False, 'dropout': 0.0, 'alpha': 0.1, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9146\n",
            "\n",
            "ğŸ” Iteration 49/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 2, 'agg_modes': ['mean', 'mean'], 'residual': False, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8743\n",
            "\n",
            "ğŸ” Iteration 50/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 4, 'agg_modes': ['mean', 'flatten'], 'residual': True, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.6337\n",
            "\n",
            "ğŸ” Iteration 51/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 4, 'agg_modes': ['mean', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8583\n",
            "\n",
            "ğŸ” Iteration 52/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.2, 'alpha': 0.4, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9011\n",
            "\n",
            "ğŸ” Iteration 53/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 8, 'agg_modes': ['flatten', 'mean'], 'residual': False, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8325\n",
            "\n",
            "ğŸ” Iteration 54/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 4, 'agg_modes': ['flatten', 'mean'], 'residual': True, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9513\n",
            "\n",
            "ğŸ” Iteration 55/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 8, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8529\n",
            "\n",
            "ğŸ” Iteration 56/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [8, 8], 'n_attention_heads': 10, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.2, 'alpha': 0.2, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8638\n",
            "\n",
            "ğŸ” Iteration 57/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [16, 16], 'n_attention_heads': 10, 'agg_modes': ['mean', 'mean'], 'residual': True, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.0, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8735\n",
            "\n",
            "ğŸ” Iteration 58/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 8, 'agg_modes': ['mean', 'mean'], 'residual': False, 'dropout': 0.0, 'alpha': 0.4, 'predictor_hidden_feats': 512, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.9357\n",
            "\n",
            "ğŸ” Iteration 59/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [64, 64], 'n_attention_heads': 12, 'agg_modes': ['flatten', 'flatten'], 'residual': False, 'dropout': 0.4, 'alpha': 0.1, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.4, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8441\n",
            "\n",
            "ğŸ” Iteration 60/60 â€” Trying params: {'n_tasks': 1, 'batch_size': 128, 'mode': 'regression', 'number_atom_features': 30, 'graph_attention_layers': [256, 256], 'n_attention_heads': 2, 'agg_modes': None, 'residual': False, 'dropout': 0.4, 'alpha': 0.4, 'predictor_hidden_feats': 128, 'predictor_dropout': 0.2, 'self_loop': True}\n",
            "ğŸ“Š Score: 0.8826\n",
            "\n",
            "âœ… Best Hyperparameters Found:\n",
            "n_tasks: 1\n",
            "batch_size: 128\n",
            "mode: regression\n",
            "number_atom_features: 30\n",
            "graph_attention_layers: [64, 64]\n",
            "n_attention_heads: 2\n",
            "agg_modes: ['flatten', 'flatten']\n",
            "residual: False\n",
            "dropout: 0.0\n",
            "alpha: 0.4\n",
            "predictor_hidden_feats: 256\n",
            "predictor_dropout: 0.0\n",
            "self_loop: True\n",
            "\n",
            "ğŸ¯ Best Pearson RÂ²: 0.9613\n",
            "\n",
            "â±ï¸ Total execution time: 165.91 minutes\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from deepchem.models import GATModel\n",
        "from deepchem.metrics import Metric, pearson_r2_score\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Define hyperparameter grid for GATModel\n",
        "param_grid = {\n",
        "    'n_tasks': [1],\n",
        "    'batch_size': [128],\n",
        "    'mode': ['regression'],\n",
        "    'number_atom_features': [30],\n",
        "    'graph_attention_layers': [[8, 8], [16, 16], [64, 64], [256, 256]],\n",
        "    'n_attention_heads': [2, 4, 6, 8, 10, 12],\n",
        "    'agg_modes': [None, ['flatten', 'mean'], ['mean', 'mean'], ['flatten', 'flatten'], ['mean', 'flatten']],\n",
        "    'residual': [True, False],\n",
        "    'dropout': [0.0, 0.2, 0.4],\n",
        "    'alpha': [0.1, 0.2, 0.4],\n",
        "    'predictor_hidden_feats': [64, 128, 256, 512],\n",
        "    'predictor_dropout': [0.0, 0.2, 0.4],\n",
        "    'self_loop': [True],\n",
        "}\n",
        "\n",
        "# Evaluation metric\n",
        "metric = Metric(pearson_r2_score, mode='regression')\n",
        "\n",
        "# Random search setup\n",
        "n_iter = 60\n",
        "results = []\n",
        "best_score = -np.inf\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    # Randomly sample parameters\n",
        "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
        "\n",
        "    print(f\"\\nğŸ” Iteration {i+1}/{n_iter} â€” Trying params: {params}\")\n",
        "\n",
        "    # Instantiate and train model\n",
        "    model = GATModel(**params)\n",
        "    model.fit(train_val_dataset, nb_epoch=30)\n",
        "\n",
        "    # Evaluate\n",
        "    scores = model.evaluate(valid_dataset, [metric])\n",
        "    score = scores[metric.name]\n",
        "    print(f\"ğŸ“Š Score: {score:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    results.append((score, params))\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = model\n",
        "        best_params = params\n",
        "\n",
        "# Report best\n",
        "print(\"\\nâœ… Best Hyperparameters Found:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "print(f\"\\nğŸ¯ Best Pearson RÂ²: {best_score:.4f}\")\n",
        "\n",
        "# End timer and report elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = (end_time - start_time) / 60  # minutes\n",
        "print(f\"\\nâ±ï¸ Total execution time: {elapsed_time:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfMqVl5QLk6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize lists to store MAE, RMSE, and R2 scores across 3 test runs\n",
        "mae_scores, rmse_scores, r2_scores = [], [], []\n",
        "\n",
        "# Perform testing three times\n",
        "for _ in range(3):\n",
        "    # Make predictions\n",
        "    pred = best_model.predict(test_dataset)\n",
        "\n",
        "    # Compute metrics\n",
        "    y_true = test_dataset.y\n",
        "    mae = mean_absolute_error(y_true=y_true, y_pred=pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=pred))\n",
        "    r2 = r2_score(y_true=y_true, y_pred=pred)\n",
        "\n",
        "    # Append scores\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "# Compute mean and std for each metric\n",
        "mean_mae, std_mae = np.mean(mae_scores), np.std(mae_scores)\n",
        "mean_rmse, std_rmse = np.mean(rmse_scores), np.std(rmse_scores)\n",
        "mean_r2, std_r2 = np.mean(r2_scores), np.std(r2_scores)\n",
        "\n",
        "# Display results\n",
        "print(f\"ğŸ“ˆ Mean MAE   : {mean_mae:.4f} Â± {std_mae:.4f}\")\n",
        "print(f\"ğŸ“ˆ Mean RMSE  : {mean_rmse:.4f} Â± {std_rmse:.4f}\")\n",
        "print(f\"ğŸ“ˆ Mean RÂ²    : {mean_r2:.4f} Â± {std_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L41dbIH770oq",
        "outputId": "767d0549-ebba-4460-a821-a4580ce60e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ˆ Mean MAE   : 0.3529 Â± 0.0000\n",
            "ğŸ“ˆ Mean RMSE  : 0.4350 Â± 0.0000\n",
            "ğŸ“ˆ Mean RÂ²    : 0.6597 Â± 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRqK7w3iLeBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}